{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35458592",
   "metadata": {},
   "source": [
    "This notebook scrapes the metadata given a piece url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fafc3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b70da192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of ids from 9_way_dataset.zip\n",
    "\n",
    "with zipfile.ZipFile('9_way_dataset.zip', 'r') as zip_file:\n",
    "    # Assuming there's only one file inside the ZIP\n",
    "    filename = zip_file.namelist()[0]\n",
    "    # Open the file inside the ZIP\n",
    "    with zip_file.open(filename) as f:\n",
    "        # Load the pickled object\n",
    "        data_9 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a18d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of ids from 100_way_dataset.zip\n",
    "\n",
    "with zipfile.ZipFile('100_way_dataset.zip', 'r') as zip_file:\n",
    "    # Assuming there's only one file inside the ZIP\n",
    "    filename = zip_file.namelist()[0]\n",
    "    # Open the file inside the ZIP\n",
    "    with zip_file.open(filename) as f:\n",
    "        # Load the pickled object\n",
    "        data_100 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8e319cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882\n"
     ]
    }
   ],
   "source": [
    "# get the list of ids from 9_way_dataset\n",
    "ids_9 = []\n",
    "for i in range(6,9):\n",
    "    for data in data_9[i]:\n",
    "        ids_9.append(data[0])\n",
    "\n",
    "ids_9 = list(set(ids_9))\n",
    "print(len(ids_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a82054a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4930\n"
     ]
    }
   ],
   "source": [
    "# get the list of ids from 100_way_dataset\n",
    "ids_100 = []\n",
    "for i in range(6,9):\n",
    "    for data in data_100[i]:\n",
    "        ids_100.append(data[0])\n",
    "\n",
    "ids_100 = list(set(ids_100))\n",
    "print(len(ids_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a0b5b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882\n",
      "882\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# get pickle file\n",
    "with open('9_way_pdf_dict.pkl', 'rb') as f:\n",
    "    dict_ids_9 = pickle.load(f)\n",
    "    \n",
    "url_dict_9={}\n",
    "not_valid_ids_9 = []\n",
    "for i in ids_9:\n",
    "    try:\n",
    "        url_dict_9[i]=dict_ids_9[i]\n",
    "    except:\n",
    "        not_valid_ids_9.append(i)\n",
    "        continue\n",
    "print(len(ids_9))\n",
    "print(len(url_dict_9))\n",
    "print(len(not_valid_ids_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dfa47aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4930\n",
      "4094\n",
      "836\n"
     ]
    }
   ],
   "source": [
    "# get pickle file\n",
    "with open('100_way_pdf_dict.pkl', 'rb') as f:\n",
    "    dict_ids_100 = pickle.load(f)\n",
    "    \n",
    "url_dict_100={}\n",
    "not_valid_ids_100 = []\n",
    "for i in ids_100:\n",
    "    try:\n",
    "        url_dict_100[i]=dict_ids_100[i]\n",
    "    except:\n",
    "        not_valid_ids_100.append(i)\n",
    "        continue\n",
    "print(len(ids_100))\n",
    "print(len(url_dict_100))\n",
    "print(len(not_valid_ids_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d617cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get page text and parse out the general information sectioin\n",
    "\n",
    "def get_page_text(url):\n",
    "    '''Retrive the text from url.'''\n",
    "    # parse web content into English text\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for any request errors\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract all text from the page, excluding script and style tags\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_between_strings_second(text, start_string, end_string):\n",
    "    '''\n",
    "    Extracts the substring between the second occurrence of start_string and end_string in text.\n",
    "    '''\n",
    "    # Find the second occurrence of the start_string (the second occurence of \"general information\" gives us our metadata)\n",
    "    if text:\n",
    "        start_index = text.find(start_string, text.find(start_string) + len(start_string))\n",
    "        if start_index == -1:  # If start_string is not found\n",
    "            return None\n",
    "\n",
    "        # Find the occurrence of the end_string\n",
    "        end_index = text.find(end_string, start_index)\n",
    "        if end_index == -1:  # If end_string is not found\n",
    "            return None\n",
    "\n",
    "        # Extract the substring between the second occurrence of start_string and end_string\n",
    "        extracted_text = text[start_index + len(start_string):end_index].strip()\n",
    "\n",
    "        return extracted_text\n",
    "\n",
    "def retrieve_general_information(url):\n",
    "    '''Retrieves the general information section from the IMSLP page of a musical score until Instrumentation.'''\n",
    "    # retrieve the general information section\n",
    "    parsed_text = extract_text_between_strings_second(get_page_text(url),'General Information','Instrumentation')\n",
    "    return parsed_text\n",
    "\n",
    "def extract_text_between_strings_new(text, start_string, end_string):\n",
    "    '''\n",
    "    Extracts the substring between start_string and end_string in text.\n",
    "    '''\n",
    "    start_index = text.find(start_string)\n",
    "    if start_index == -1:  # If start_string is not found\n",
    "        return None\n",
    "\n",
    "    # Find the occurrence of the end_string\n",
    "    end_index = text.find(end_string, start_index)\n",
    "    if end_index == -1:  # If end_string is not found\n",
    "        return None\n",
    "\n",
    "    # Extract the substring between the second occurrence of start_string and end_string\n",
    "    extracted_text = text[start_index + len(start_string):end_index].strip()\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2344389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find general information section\n",
    "\n",
    "def get_gen_info(url_dict):\n",
    "    attribute_list = ['Work Title', 'Alt\\nernative\\n.\\nTitle', 'Name Translations', 'Name Aliases', 'Authorities', 'Composer',\n",
    "                            'Opus/Catalogue Number', 'I-Catalogue Number', 'Key', 'Movements/Sections', 'Year/Date of Composition', \n",
    "                            'First Pub\\nlication', 'Librettist', 'Language', 'Copyright Information','Dedication', 'Average Duration', 'Composer Time Period', \"Piece Style\"]\n",
    "    list_of_deletable_headings = ['Opus/Catalogue Number','I-Catalogue Number','Movements/Sections','Year/Date of Composition','First Pub\\nlication','Average Duration','Composer Time Period']\n",
    "    bad_heading_pattern = r'^.*\\n'\n",
    "\n",
    "    info = {}\n",
    "    for pdf_id, url in url_dict.items():\n",
    "        parsed_text = retrieve_general_information(url)\n",
    "\n",
    "        if parsed_text:\n",
    "\n",
    "            info_dic = {}\n",
    "\n",
    "            present_attributes = []\n",
    "            # check which attributes are present in the parsed text\n",
    "            for attribute in attribute_list:\n",
    "                if parsed_text.find(attribute) != -1:\n",
    "                    present_attributes.append(attribute)\n",
    "\n",
    "            # extract the text for each present attribute\n",
    "            for attribute in attribute_list[:-1]:\n",
    "                if attribute in present_attributes:\n",
    "                    info_dic[attribute] = extract_text_between_strings_new(parsed_text, attribute, present_attributes[present_attributes.index(attribute) + 1] if present_attributes.index(attribute) != len(present_attributes) - 1 else \"Romantic\")\n",
    "\n",
    "            info_dic['Piece Style'] = parsed_text[parsed_text.find('Piece Style')+12:] # extract the piece style from the end of the general information section\n",
    "            info_dic['Instrumentation'] = 'Piano'\n",
    "            info_dic['url'] = url\n",
    "\n",
    "            if 'Alt\\nernative\\n.\\nTitle' in present_attributes:\n",
    "                alt_title = info_dic['Alt\\nernative\\n.\\nTitle']\n",
    "                info_dic['Alternative Title'] = alt_title\n",
    "                del info_dic['Alt\\nernative\\n.\\nTitle']\n",
    "\n",
    "            if 'First Pub\\nlication' in present_attributes:\n",
    "                alt_title = info_dic['First Pub\\nlication']\n",
    "                info_dic['First Publication'] = alt_title\n",
    "                del info_dic['First Pub\\nlication']\n",
    "\n",
    "            for heading in list_of_deletable_headings:\n",
    "                if heading in info_dic.keys():\n",
    "                    try:\n",
    "                        info_dic[heading] = re.sub(bad_heading_pattern, '', info_dic[heading])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            info[pdf_id] = info_dic\n",
    "\n",
    "        else:\n",
    "            print(\"String not found or invalid input.\")\n",
    "            \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c679c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_9 = get_gen_info(url_dict_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64bb5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_100 = get_gen_info(url_dict_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e91ba23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pickle file from the dictionary\n",
    "with open('9_way_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(info_9, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbaca90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('100_way_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(info_100, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fdb22f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 882 entries, 53763 to 20432\n",
      "Data columns (total 20 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   Work Title                882 non-null    object\n",
      " 1   Composer                  882 non-null    object\n",
      " 2   Opus/Catalogue Number     870 non-null    object\n",
      " 3   I-Catalogue Number        881 non-null    object\n",
      " 4   Key                       564 non-null    object\n",
      " 5   Movements/Sections        846 non-null    object\n",
      " 6   Composer Time Period      882 non-null    object\n",
      " 7   Piece Style               882 non-null    object\n",
      " 8   Instrumentation           882 non-null    object\n",
      " 9   url                       882 non-null    object\n",
      " 10  Alternative Title         882 non-null    object\n",
      " 11  Year/Date of Composition  762 non-null    object\n",
      " 12  Name Translations         322 non-null    object\n",
      " 13  Authorities               309 non-null    object\n",
      " 14  Average Duration          127 non-null    object\n",
      " 15  First Publication         740 non-null    object\n",
      " 16  Dedication                240 non-null    object\n",
      " 17  Name Aliases              240 non-null    object\n",
      " 18  Language                  4 non-null      object\n",
      " 19  Copyright Information     3 non-null      object\n",
      "dtypes: object(20)\n",
      "memory usage: 144.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe from the dictionary\n",
    "df_9 = pd.DataFrame.from_dict(info_9, orient='index')\n",
    "df_9.info()\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df_9.to_csv('9_way_metadata.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
