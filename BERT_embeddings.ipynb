{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pqa-7WXBAw8q"
   },
   "source": [
    "# 1. Loading Pre-Trained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSXImOxMPdNg"
   },
   "source": [
    "Now let's import pytorch, the pretrained BERT model, and a BERT tokenizer.\n",
    "\n",
    "We'll explain the BERT model in detail in a later tutorial, but this is the pre-trained model released by Google that ran for many, many hours on Wikipedia and [Book Corpus](https://arxiv.org/pdf/1506.06724.pdf), a dataset containing +10,000 books of different genres. This model is responsible (with a little modification) for beating NLP benchmarks across a range of tasks. Google released a few variations of BERT models, but the one we'll use here is the smaller of the two available sizes (\"base\" and \"large\") and ignores casing, hence \"uncased.\"\"\n",
    "\n",
    "`transformers` provides a number of classes for applying BERT to different tasks (token classification, text classification, ...). Here, we're using the basic `BertModel` which has no specific output task--it's a good choice for using BERT just to extract embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lJEnBJ3gHTsQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azhu/anaconda3/envs/baselines/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azhu/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/azhu/anaconda3/envs/baselines/lib/python3.11/site-packages/torch/cuda/__init__.py:740: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tlv3VlPnKKHN"
   },
   "source": [
    "# 2. Input Formatting\n",
    "Because BERT is a pretrained model that expects input data in a specific format, we will need:\n",
    "\n",
    "1. A **special token, `[SEP]`,** to mark the end of a sentence, or the separation between two sentences\n",
    "2. A **special token, `[CLS]`,** at the beginning of our text. This token is used for classification tasks, but BERT expects it no matter what your application is.\n",
    "3. Tokens that conform with the fixed vocabulary used in BERT\n",
    "4. The **Token IDs** for the tokens, from BERT's tokenizer\n",
    "5. **Mask IDs** to indicate which elements in the sequence are tokens and which are padding elements\n",
    "6. **Segment IDs** used to distinguish different sentences\n",
    "7. **Positional Embeddings** used to show token position within the sequence\n",
    "\n",
    "Luckily, the `transformers` interface takes care of all of the above requirements (using the `tokenizer.encode_plus` function).\n",
    "\n",
    "Since this is intended as an introduction to working with BERT, though, we're going to perform these steps in a (mostly) manual way.\n",
    "\n",
    "> *For an example of using `tokenizer.encode_plus`, see the next post on Sentence Classification [here](http://mccormickml.com/2019/07/22/BERT-fine-tuning/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diVtyCJCurxJ"
   },
   "source": [
    "## Tokenizing and Extracting Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Pg0P9rFxJwwp",
    "outputId": "a8314f8a-e95a-43e1-db4c-395f0334b942"
   },
   "outputs": [],
   "source": [
    "all_sentence_embeddings = []\n",
    "\n",
    "for text in open('metadata_string.txt'):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    # Tokenize our sentence with the BERT tokenizer.\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(tokenized_text)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Concatenate the tensors for all layers\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # For each token in the sentence\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:],dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    # Extracting sentence embeddings\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    all_sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "np.save(\"/home/azhu/ttmp/MMCSR/metadata_embeddings/sentence_embeddings.npy\",all_sentence_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "baselines",
   "language": "python",
   "name": "baselines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
